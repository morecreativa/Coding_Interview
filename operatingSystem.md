# Part 1. Operating System

## 1. 프로세스
1) 프로그램이 실행되는 단위, 실행중인 프로그램을 뜻함. 
2) **디스크로부터 메모리에 적재되어 CPU의 할당을 받을 수 있는 것**
3) OS로부터 주소공간, 파일, 메모리 등을 할당받는다.
4) 스택 섹션 : 함수의 매개변수, 복귀 주소와 로컬 변수 + 임시 자료
5) 데이터 섹션 : 전역 변수
6) 힙 섹션 : **동적할당** 되는 변수가 저장되는 메모리

</br>

## 2. 프로세스 제어블록
1) 프로세스에 대한 중요한 정보를 저장하는 운영체제의 자료구조 
2) 프로세스는 CPU를 할당받아 작업을 처리하면서도 **프로세스 전환 발생 시** CPU를 반환해야한다. 이때 작업의 진행상황을 PCB에 저장함.
3) **프로세스 식별자, 프로세스 상태, 프로그램 카운터, CPU레지스터, 스케줄링 정보, 메모리 관리 정보, 입출력 상태 정보, 어카운팅 정보(사용된 CPU시간, 시간제한, 계정번호)**

</br>

## 3. 스레드
1) **프로세스가 실행되는 단위**
2) 한 프로세스 내에서 실행되는 여러가지 작업으로 자원과 주소공간을 공유할 수 있음
*  같은 프로세스 내에서 코드 섹션, 스택섹션, 운영체제 자원을 공유함
3) 스레드 ID, 프로그램 카운터, 레지스터 집합, 스택으로 구성
4) 스레드에 독립적으로 스택과 PC를 할당하는 이유가 뭐임둥?
* 스택 : 함수의 매개변수, 로컬 변수와 복귀 주소 등을 저장하는 메모리 공간인데, **"스택 메모리가 독립적이다" -> "실행할 수 있는 독립적인 함수 호출이 가능하다" 이므로 독립적인 실행 흐름이 추가되는 것임.** 따라서 스레드의 정의에 맞게 독립적인 실행흐름을 추가하기 위한 조건이 되는 것임.
* PC(프로그램 카운터) : 스레드는 CPU를 할당받았다가 선점 스케줄러의 영향으로 반환할 수 있는데 **명령어 수행이 연속적이지 않다. 따라서 진행상황을 기억할 필요가 있다.** 따라서 독립적으로 할당되는 거임메.

</br>

## 4. 멀티 스레드 
1) **하나의 프로세스를 다수의 실행단위로 나누어 자원을 공유하고 자원의 생성과 관리의 중복을 피하여 수행능력을 향상시키는 행위!!!**
2) 메모리 공간과 시스템 자원 소모가 줄어들게 됨.
3) 전역 변수의 공간 + Heap 영역을 통하여 
4) 스레드의 Context switch는 프로세스의 Context switch보다 빠르다. 캐시 메모리를 비울 필요가 없거든!
5) **문제점!!**
 * 멀티 프로세스간에는 공유할 자원이 없어 동일한 자원에 접근하는 경우가 없지만, 멀티 스레드에서는 주의해야함. 
 * **서로 다른 스레드가 Data, Heap 영역을 공유하기 때문에 어떤 스레드가 사용중인 변수나 자료구조에 접근하여 엉뚱한 값을 읽어오거나 수정될 수 있음!**
 * 그래서 필요한게 동기화 작업임 -> 과도한 락은 병목현상을 불러일으킬 수 있음
 * **동기화**를 통해 작업 처리 순서를 컨트롤 + 공유 자원에 대한 접근을 컨트롤

</br>

## 5. 스케줄러 
1) 스케줄러에는 Job큐, Ready큐, Device I/O 큐가 존재함
* Job큐 : 현재 시스템 내에 있는 모든 프로세스의 집합
* Ready큐 : 현재 메모리 내에 있으면서 CPU의 할당받기를 기다리는 상태
* Device 큐 : Device I/O 작업을 대기하고 있는 프로세스의 집합
2) 장기스케줄러 : 메모리는 한정되어 있는 상태에서, 한꺼번에 많은 프로세스들이 메모리에 올라올 때 대용량 메모리(일반적으로 디스크, pool)에 저장이 된다. 이때 어떤 프로세스에 메모리를 할당하여 ready queue로 보낼지 결정하는 역할을 함.
* 메모리와 디스크 사이의 스케줄링을 담당
* 프로세스에 메모리+각종 리소스를 할당
* Degree of Multiprogramming 제어
* 프로세스의 상태 new -> ready
3) 단기스케줄러 : CPU와 메모리 사이의 스케줄링을 담당한다.
* Ready Queue에 존재하는 프로세스 중 어떤 프로세스를 running 시킬 지 결정하는 역할
* 프로세스에 CPU를 할당함.
* 프로세스의 상태 new -> ready -> waiting -> ready
4) 중기스케줄러 : 여유 공간을 마련하기 위해 프로세스를 통째로 메모리에서 디스크로 쫓아냄(Swapping)
* 프로세스에게서 메모리를 deallocate
* 메모리에 너무 많은 프로그램이 올라가는 것을 조절하는 스케줄러
* 프로세스의 상태 raedy -> suspended

</br>


## 6. CPU 스케줄러
* **스케줄링 대상은 Ready Queue에 있는 프로세스들이다.**
1) FCFS : First Come First Served -> 비선점형, 먼저온거 걍 먼처함 / convoy effect
2) SJF : Shortest Job First -> CPU burst time이 짧은 프로세스에게 선할당, starvation
3) SRT : Shortest Remaining time First -> 새로운 프로세스, 새로운 스케줄링, startvation, Have difficult in counting CPU burst time
4) Priority Scheduling : 선점형 / 비선점형으로 나뉨
* 선점형 : 더 높은 우선순위가 나타나면 그냥 금마한테 바로 CPU줘버림
* 비선점형 : 더 높은 우선순위가 나타나면 Ready Queue의 Head에 넣어버림
* starvation, Indefinite Blokcing <- Aging 기법으로 우선순위를 높여줌.
5) Round Robin : 현대적인 CPU 스케줄링, 각자의 time Quantum을 가짐. 
* Response Time이 빨라짐. 
* 프로세스가 기다리는 시간이 CPU를 사용할 만큼 증가함.
* Time Quantum이 너무 커지면 FCFS와 같아짐. 또 너무 작아지면 스케줄링 알고리즘의 목적에는 이상적이지만 잦은 Context Switch로 overhead가 발생함.
</br>


## 7. 동기와 비동기의 차이
1) 메소드를 실행시킴으로써 **동시에** 반환값이 기대되는 경우를 동기
2) 그렇지 않은 경우에는 비동기 -> 동시에: 실행되었을 때 값이 반환되기 전까지 blocking 되어 있는 상태
3) 비동기의 경우 **blocking 되지 않고 이벤트 큐에 넣거나 백그라운드 thread에 해당 task를 위임하고 바로 다음 코드를 실행하기 때문에 기대되는 값이 바로 반환되지 않음**

</br>

## 8. 프로세스 동기화
1) **임계영역**: 멀티 스레딩의 문제점에서 나오듯, 동일한 자원을 동시에 접근하는 작업을 실행하는 코드 영역을 Critical Section이라 칭함.
2) 프로세스들이 Critical Section을 함께 사용할 수 있는 프로토콜을 설계하는 행위 -> Critical Section Problem
3) Requirements(Basic condition to solve Critical Section)
* Mutual Exclusion : **상호배제** 프로세스가 실행중이라면 다른 프로세스는 Critical Section에서 실행 불가
* Progress : 1. Critical Section에서 실행중인 프로세스가 없으며 2. 현재 별도의 동작이 없는 프로세스들만 Critical Section 진입 후보로서 참여될 수 있다.
* Bounded Waiting : 특정 프로세스가 Critical Section에 진입 신청 후, 다른 프로세스들이 Critical Section에 진입하는 횟수는 제한이 있어야한다.
* **하나 더있는데 이거 찾아봐야돼!!**
4) **Lock**: 하드웨어 기반 해결책으로 동시에 공유 자원에 접근하는 것을 막기 위해 Critical Section에 진입하는 프로세스는 Lock을 획득하고 빠져나올 때에는 Lock을 방출한다.
5) **Semaphore**: Critical Section 문제를 해결하기 위한 동기화 도구 카운팅 세마포-> **가용한 개수를 가진 자원**에 대한 접근 제어용으로 사용되며, 세마포어는 **자원의 개수**로 초기화 된다. 자원을 사용하면 세마포어 -- 방출하면 ++ ㅇㅋ? / 이진 세마포(**MUTEX**) -> 0과 1 사이의 값만 가능하며, 다중 프로세스들 사이의 Critical Section 문제를 해결하기 위해 사용한다.
6) 세마포어 단점: Busy Waiting->Spin lock이라고 불리는 Semaphore 초기 버전에서 Critical Section에 진입해야하는 프로세스는 진입 코드를 계속 실행시켜 CPU 시간을 낭비하였다. 일반적으로는 Semaphore에서 Ciritical Section에 진입을 시도했지만 실패한 프로세스에 대해 Block시킨 뒤, Ciritical Section에 자리가 날 때 다시 깨우는 방식을 사용한다. Busy Waiting으로 인한 시간낭비 문제가 해결됨!

</br>

## 9. 메모리 관리전략
1) 각각의 프로세스는 독립된 메모리 공간을 가짐. 서로 다른 메모리 공간에 접근할 수 없는 제한이 걸려있다.
2) **운영체제만이 운영체제 메모리 영역과 사용자 메모리 영역의 접근에 제약을 받지 않는다.**
3) **Swapping**: 메모리의 관리를 위한 기법. Round-robin과 같은 스케줄링의 방식으로 다중 프로그래밍 환경에서 CPU 할당 시간이 끝난 프로세스의 메모리를 보조 기억장치로 보내고, 다른 프로세스의 메모리를 불러 들일 수 있다. swap in은 보조 기억 장치에서 주 기억 장치로 불러오는 과정, 보조 기억 장치로 내보내는 과정을 swap-out
4) **단편화**: Fragmentation이라고도 불림. 프로세스들이 메모리에 적재되고 제거되는 일이 반복되면, 프로세스가 차지하는 메모리 틈 사이에 사용하지 못할 만큼의 작은 자유공간들이 늘어나게 된다. 
* 내부 단편화: 메모리 자유분할 공간이 10,000B일때 프로세스가 9998B를 사용하게 되면 2B라는 차이가 존재하게 되고, 이 현상을 내부 단편화라고 한다.
* 외부 단편화: 물리 메모리에서 사이사이 남는 공간들을 모두 합치면 충분한 공간이 되는 부분들이 **분산되어 있을 때** 발생한다.
5) **페이징**: Paging, 하나의 프로세스가 사용하는 메모리 공간이 연속적이어야 한다는 제약을 없앤 메모리 관리 방법. **외부 단편화와 압축 작업을 해소하기 위한 방법론**. 물리 메모리는 Frame에 고정, 논리 메모리(프로세스가 점유하는)는 페이지라 불리는 고정 크기의 블록으로 분리된다.
* 논리 메모리는 물리 메모리에 저장될 때 연속되어 저장될 필요가 없고 메모리의 남는 프레임에 적절히 배치된다. 따라서 외부 단편화를 해결할 수 있다.
* 하나의 프로세스가 사용하는 공간은 여러개의 페이지로 나누어 관리되고(논리 메모리에서), 개별 페이지는 순서에 상관없이 물리 메모리에 있는 프레임에 mapping 되어 저장된다고 볼 수 있음.
* 그런데 내부 단편화 문제의 비중이 늘어나게 됨!
6)**Segmentation**: 페이징에서처럼 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌, 서로 다른 크기의 논리적 단위(Segment)로 분할 사용자가 두 개의 주소로 지정
* Segment Number + Vector
* 세그먼트의 기준(세그먼트의 시작 물리 Address) + 한계(세그먼트의 길이)를 저장
</br>


## 10. 가상 메모리
1) 다중 프로그래밍을 실현하기 위해서는 많은 프로세스들을 동시에 메모리에 올려두어야 한다. 가상 메모리는 **프로세스 전체가 메모리 내에 올라오지 않더라도 실행이 가능하도록 하는 기법이다**.
2) 프로그램이 물리 메모리보다 커도 된다는 주요 장점이 있다.
3) **개발배경**: 실행되는 **코드의 전부를 물리 메모리에 존재시켜야**했고, **메모리 용양보다 큰 프로그램은 실행 시킬 수 없었다.** 여러 프로그램을 동시에 메모리에 올리기에는 용량의 한계, 페이지 교체등의 성능 이슈가 발생하게 된다.</br>
**프로그램의 일부분만 메모리에 올릴 수 있다면...**
* 물리 메모리의 크기에 제약을 받지 않는다.
* 더 많은 프로그램을 동시에 실행시킬 수 있다. 이에 따라 **응답시간**은 유지되고, **CPU이용률 + 처리율**은 높아진다.
* swap에 필요한 입출력이 줄어들기 때문에 프로그램들이 빠르게 실행된다.
4) 실제의 물리 메모리 개념과 사용자의 논리 메모리 개념을 분리한 것이다. 
</br>


## 11. 캐시
